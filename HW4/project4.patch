--- slob.c	2018-06-08 09:27:40.563016811 -0700
+++ slob_New.c	2018-06-08 09:00:32.954194520 -0700
@@ -67,9 +67,10 @@
 #include <linux/rcupdate.h>
 #include <linux/list.h>
 #include <linux/kmemleak.h>
-
+#include <linux/syscalls.h>
 #include <trace/events/kmem.h>
 
+#include <linux/linkage.h>
 #include <linux/atomic.h>
 
 #include "slab.h"
@@ -87,6 +88,9 @@
 typedef s32 slobidx_t;
 #endif
 
+unsigned long slobCount = 0;
+unsigned long freeUnits = 0;
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -111,13 +115,13 @@
 
 static void set_slob_page_free(struct page *sp, struct list_head *list)
 {
-	list_add(&sp->lru, list);
+	list_add(&sp->list, list);
 	__SetPageSlobFree(sp);
 }
 
 static inline void clear_slob_page_free(struct page *sp)
 {
-	list_del(&sp->lru);
+	list_del(&sp->list);
 	__ClearPageSlobFree(sp);
 }
 
@@ -133,7 +137,6 @@
 	struct rcu_head head;
 	int size;
 };
-
 /*
  * slob_lock protects all slob allocator structures.
  */
@@ -268,10 +271,13 @@
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
+  struct page *tempSP = NULL;
 	struct list_head *prev;
 	struct list_head *slob_list;
+  struct list_head *temp;
 	slob_t *b = NULL;
 	unsigned long flags;
+  freeUnits = 0;
 
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
@@ -282,7 +288,7 @@
 
 	spin_lock_irqsave(&slob_lock, flags);
 	/* Iterate through each partially free page, try to find room */
-	list_for_each_entry(sp, slob_list, lru) {
+	list_for_each_entry(sp, slob_list, list) {
 #ifdef CONFIG_NUMA
 		/*
 		 * If there's a node specification, search for a partial
@@ -295,22 +301,31 @@
 		if (sp->units < SLOB_UNITS(size))
 			continue;
 
-		/* Attempt to alloc */
-		prev = sp->lru.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
-
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+        if (tempSP == NULL)
+                tempSP = sp;
+        // get lowest page
+        if (sp->units < tempSP->units)
+                tempSP = sp;
 	}
-	spin_unlock_irqrestore(&slob_lock, flags);
+    if(tempSP != NULL)
+    {
+        b = slob_page_alloc(tempSP, size, align);
+    }
+    //freeing space
+    temp = &free_slob_small;
+    list_for_each_entry(sp, temp, list) {
+        freeUnits += sp->units;
+    }
+    temp = &free_slob_medium;
+    list_for_each_entry(sp, temp, list) {
+        freeUnits += sp->units;
+    }
+    temp = &free_slob_large;
+    list_for_each_entry(sp, temp, list) {
+        freeUnits += sp->units;
+    }
 
+	spin_unlock_irqrestore(&slob_lock, flags);
 	/* Not enough space: must allocate a new page */
 	if (!b) {
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
@@ -322,12 +337,14 @@
 		spin_lock_irqsave(&slob_lock, flags);
 		sp->units = SLOB_UNITS(PAGE_SIZE);
 		sp->freelist = b;
-		INIT_LIST_HEAD(&sp->lru);
+		INIT_LIST_HEAD(&sp->list);
 		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
 		set_slob_page_free(sp, slob_list);
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+        // Increment when allocate a page
+        slobCount++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +379,8 @@
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+        // Decrement when delocate the page
+        slobCount--;
 		return;
 	}
 
@@ -468,6 +487,7 @@
 }
 EXPORT_SYMBOL(__kmalloc);
 
+#ifdef CONFIG_TRACING
 void *__kmalloc_track_caller(size_t size, gfp_t gfp, unsigned long caller)
 {
 	return __do_kmalloc_node(size, gfp, NUMA_NO_NODE, caller);
@@ -480,6 +500,7 @@
 	return __do_kmalloc_node(size, gfp, node, caller);
 }
 #endif
+#endif
 
 void kfree(const void *block)
 {
@@ -618,10 +639,11 @@
 	return 0;
 }
 
-int __kmem_cache_shrink(struct kmem_cache *d)
+int kmem_cache_shrink(struct kmem_cache *d)
 {
 	return 0;
 }
+EXPORT_SYMBOL(kmem_cache_shrink);
 
 struct kmem_cache kmem_cache_boot = {
 	.name = "kmem_cache",
@@ -630,6 +652,16 @@
 	.align = ARCH_KMALLOC_MINALIGN,
 };
 
+asmlinkage long sys_slob_used(void) {
+		// return the page size * total page count
+    long slob_used = SLOB_UNITS(PAGE_SIZE) * slobCount;
+    return slob_used;
+}
+
+asmlinkage long sys_slob_free(void) {
+    return freeUnits;
+}
+
 void __init kmem_cache_init(void)
 {
 	kmem_cache = &kmem_cache_boot;
